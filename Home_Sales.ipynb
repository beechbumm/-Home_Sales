{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 22 Challenge\n",
        "## Home Sales Data"
      ],
      "metadata": {
        "id": "RZVT4uhsN8Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qTf5VT7OBbEU",
        "outputId": "dd927fe0-ec4a-4e91-b3d6-6bfbfc87000e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=9929c116ee7b50534ca17a840d69d3fbfa0eaa8245a365a022f89fecdc719839\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YYpeyGSJBjjc",
        "outputId": "3fac2f9a-3e77-46e2-b5c7-84f5dc43ba95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a_KW73O2e3dw"
      },
      "outputs": [],
      "source": [
        "# Import findspark and initialize.\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the description of how It works and how I understand.\n",
        "1. **Import necessary libraries:**\n",
        "   - I import the `os` library to interact with the operating system.\n",
        "\n",
        "2. **Set the Spark version:**\n",
        "   - I find the latest version of Spark 3.x from the Apache website.\n",
        "   - I then set the Spark version to `spark-3.4.0` and store it in the environment variable `SPARK_VERSION`.\n",
        "\n",
        "3. **Install Spark and Java:**\n",
        "   - I update the package lists using `apt-get update`.\n",
        "   - I install Java Development Kit (JDK) version 11, which is required to run Spark.\n",
        "   - I download the specified version of Spark from the Apache website using `wget`.\n",
        "   - I extract the downloaded Spark files using `tar`.\n",
        "   - I install the `findspark` Python package, which helps me to easily use Spark in Python.\n",
        "\n",
        "4. **Set environment variables for Java and Spark:**\n",
        "   - I set the `JAVA_HOME` environment variable to the path where Java 11 is installed.\n",
        "   - I set the `SPARK_HOME` environment variable to the path where the downloaded and extracted Spark files are located.\n",
        "\n",
        "5. **Initialize SparkSession:**\n",
        "   - I import `findspark` to make sure that the system can find the Spark installation.\n",
        "   - I use `findspark.init()` to initialize the Spark environment, allowing me to create a SparkSession in Python."
      ],
      "metadata": {
        "id": "WEYunGqIvmez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.4.0'\n",
        "spark_version = 'spark-3.4.0'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "C6-xHXgQCEuc",
        "outputId": "ab62b3e9-0b6d-41ea-99c3-29713e3bf0f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.org (108.139.15.54)] [C\r                                                                                                    \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r                                                                                                    \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [2 InRelease 14.2 kB/129 kB 11%] [Connected to r2u.stat.illinois.edu (192.1\r                                                                                                    \rGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [4 InRelease 12.7 kB/128 kB 10%] [2 InRelease 14.2 kB/129 kB 11%] [Connected to r2u.stat.illinois\r                                                                                                    \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [4 InRelease 47.5 kB/128 kB 37%] [2 InRelease 73.5 kB/129 kB 57%] [Connected to r2u.stat.illinois\r0% [4 InRelease 47.5 kB/128 kB 37%] [2 InRelease 73.5 kB/129 kB 57%] [Connected to r2u.stat.illinois\r0% [4 InRelease 76.4 kB/128 kB 60%] [Waiting for headers] [Connected to ppa.launchpadcontent.net (18\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.8\r                                                                                                    \rHit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [921 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,152 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,841 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,422 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,954 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,441 kB]\n",
            "Fetched 13.0 MB in 2s (6,584 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "tar: spark-3.4.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk\n",
        "!wget -q http://apache.mirrors.tds.net/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install findspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2Q2-k69AF_D7",
        "outputId": "498ff729-570e-413a-c06f-9f332e94db0f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libice-dev libsm-dev libxkbfile1 libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre x11-utils\n",
            "Suggested packages:\n",
            "  libice-doc libsm-doc libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libice-dev libsm-dev libxkbfile1 libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk openjdk-11-jre\n",
            "  x11-utils\n",
            "0 upgraded, 14 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 5,517 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.24+8-1ubuntu3~22.04 [214 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.24+8-1ubuntu3~22.04 [1,336 kB]\n",
            "Fetched 5,517 kB in 0s (13.0 MB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 123594 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../04-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../06-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../07-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../08-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libice-dev:amd64.\n",
            "Preparing to unpack .../09-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n",
            "Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Selecting previously unselected package libsm-dev:amd64.\n",
            "Preparing to unpack .../10-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n",
            "Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../12-openjdk-11-jre_11.0.24+8-1ubuntu3~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../13-openjdk-11-jdk_11.0.24+8-1ubuntu3~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "tar: spark-3.4.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I import SparkSession from the pyspark.sql module, which I need to create a Spark session.\n",
        "\n",
        "I also import the time module so I can measure how long certain operations take.\n",
        "\n",
        "I create a Spark session using SparkSession.builder. This session allows me to interact with Spark and run SQL queries.\n",
        "\n",
        "I set the application name to \"SparkSQL\" using .appName(\"SparkSQL\"), which helps me identify my application.\n",
        "\n",
        "I use .getOrCreate() to either get an existing Spark session or create a new one if none is currently running.\n",
        "\n",
        "This Spark session is essential for running SQL queries and processing data with Spark."
      ],
      "metadata": {
        "id": "OnpYrpchv_6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I import SparkFiles from the pyspark module to work with files in Spark.\n",
        "\n",
        "I set the url variable to the location of a CSV file stored in an AWS S3 bucket. The file contains the data I want to analyze, specifically the \"home_sales_revised.csv\" file.\n",
        "\n",
        "This URL points to the dataset I will load into a DataFrame for further processing in Spark."
      ],
      "metadata": {
        "id": "xmO-o2WBHEN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wOJqxG_RPSwp"
      },
      "outputs": [],
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I add the CSV file from the specified URL to Spark's context using spark.sparkContext.addFile(url). This makes the file available to my Spark job.\n",
        "\n",
        "I read the CSV file into a DataFrame named df using spark.read.csv(). I set header=True to treat the first row as column names and inferSchema=True to automatically determine the data types of each column.\n",
        "\n",
        "I create a temporary view of the DataFrame using df.createOrReplaceTempView(\"home_sales\"). This allows me to run SQL queries on the DataFrame as if it were a SQL table, with the name \"home_sales\".\n",
        "\n",
        "This process lets me load the data into Spark and create a temporary SQL-like table for querying."
      ],
      "metadata": {
        "id": "RnAe6s6UxDej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
        "df.createOrReplaceTempView(\"home_sales\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rC6yoopxHOk0",
        "outputId": "9303e65a-4811-4c24-eed0-ebf55bb86988"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- date_built: integer (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- bedrooms: integer (nullable = true)\n",
            " |-- bathrooms: integer (nullable = true)\n",
            " |-- sqft_living: integer (nullable = true)\n",
            " |-- sqft_lot: integer (nullable = true)\n",
            " |-- floors: integer (nullable = true)\n",
            " |-- waterfront: integer (nullable = true)\n",
            " |-- view: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I use `spark.sql()` to write an SQL query that calculates the average price of four-bedroom houses sold each year.\n",
        "\n",
        "- In the query, I first extract the year from the `date` column using `year(date)`, then calculate the average price of homes using `AVG(price)`.\n",
        "\n",
        "- I use `ROUND(AVG(price), 2)` to round the average price to two decimal places.\n",
        "\n",
        "- I filter the results to include only rows where `bedrooms = 4`.\n",
        "\n",
        "- I group the results by `year(date)` to get the average price for each year and order the results by `year`.\n",
        "\n",
        "- Finally, I store the result in the `result` variable and display it using `result.show()`.\n"
      ],
      "metadata": {
        "id": "3UrHnjlDxJgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L6fkwOeOmqvq",
        "outputId": "fb91d733-8e86-4689-fa96-b6d30172f592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2019| 300263.7|\n",
            "|2020|298353.78|\n",
            "|2021|301819.44|\n",
            "|2022|296363.88|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Calculate the average price for a four-bedroom house sold per year, rounded to two decimal places.\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT year(date) as year,ROUND(AVG(price), 2) as avg_price FROM home_sales\n",
        "    WHERE bedrooms = 4\n",
        "    GROUP BY year(date) ORDER BY year\n",
        "\"\"\")\n",
        "\n",
        "# Show the result\n",
        "result.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I write an SQL query using `spark.sql()` to calculate the average price of homes built in each year that have 3 bedrooms and 3 bathrooms.\n",
        "\n",
        "- In the query, I select `date_built` as `year` and calculate the average price of homes using `AVG(price)`.\n",
        "\n",
        "- I round the average price to two decimal places with `ROUND(AVG(price), 2)`.\n",
        "\n",
        "- I filter the results to include only homes with 3 bedrooms and 3 bathrooms by using `WHERE bedrooms = 3 AND bathrooms = 3`.\n",
        "\n",
        "- I group the results by `date_built` to get the average price for each year the home was built, and I order the results by `date_built`.\n",
        "\n",
        "- I store the query result in the `result` variable and display it with `result.show()`.\n"
      ],
      "metadata": {
        "id": "VuAxMV-WxOwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Calculate the average price of a home for each year it was built,\n",
        "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places.\n",
        "result = spark.sql(\"\"\"SELECT date_built as year,ROUND(AVG(price), 2) as avg_price FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3\n",
        "GROUP BY date_built ORDER BY date_built\"\"\")\n",
        "\n",
        "# Show the result\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xTpVoHEDHUTh",
        "outputId": "6a042455-29bd-48ba-e28a-696fa28bbb29"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2010|292859.62|\n",
            "|2011|291117.47|\n",
            "|2012|293683.19|\n",
            "|2013|295962.27|\n",
            "|2014|290852.27|\n",
            "|2015| 288770.3|\n",
            "|2016|290555.07|\n",
            "|2017|292676.79|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I create an SQL query using `spark.sql()` to calculate the average price of homes built in each year that meet specific criteria.\n",
        "\n",
        "- In the query, I select `date_built` as `year` and calculate the average price of homes using `AVG(price)`.\n",
        "\n",
        "- I round the average price to two decimal places with `ROUND(AVG(price), 2)`.\n",
        "\n",
        "- I filter the results to include only homes with 3 bedrooms, 3 bathrooms, 2 floors, and a living area of at least 2,000 square feet by using `WHERE` conditions.\n",
        "\n",
        "- I group the results by `date_built` to get the average price for each year the home was built, and I order the results by `date_built`.\n",
        "\n",
        "- I store the query result in the `result` variable and display it with `result.show()`.\n"
      ],
      "metadata": {
        "id": "DflQS3Rixdhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Calculate the average price of a home for each year it was built,\n",
        "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet, rounded to two decimal places.\n",
        "result = spark.sql(\"\"\"SELECT date_built as year, ROUND(AVG(price), 2) as avg_price FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3\n",
        "AND floors = 2 AND sqft_living >= 2000\n",
        "GROUP BY date_built ORDER BY date_built\"\"\")\n",
        "\n",
        "# Show the result\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LgzN3uU_HXiZ",
        "outputId": "efce3d35-605c-4e54-d476-b23b5b99e496"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2010|285010.22|\n",
            "|2011|276553.81|\n",
            "|2012|307539.97|\n",
            "|2013|303676.79|\n",
            "|2014|298264.72|\n",
            "|2015|297609.97|\n",
            "|2016| 293965.1|\n",
            "|2017|280317.58|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I start by importing the `time` module to measure how long the query takes to run.\n",
        "\n",
        "- I use `time.time()` to get the current time before running the query, storing it in `start_time`.\n",
        "\n",
        "- I write an SQL query with `spark.sql()` to calculate the average price of homes for each `view` rating.\n",
        "\n",
        "- I use `ROUND(AVG(price), 2)` to round the average price to two decimal places.\n",
        "\n",
        "- I filter the results using `HAVING AVG(price) >= 350000` to include only view ratings with an average home price of $350,000 or more.\n",
        "\n",
        "- I order the results by `view` in descending order using `ORDER BY view DESC`.\n",
        "\n",
        "- I store the query results in the `result` variable and display them with `result.show()`.\n",
        "\n",
        "- After running the query, I get the current time again using `time.time()` and store it in `end_time`.\n",
        "\n",
        "- I calculate the runtime by subtracting `start_time` from `end_time` and print the result.\n"
      ],
      "metadata": {
        "id": "8_2lLQPJxjDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "93dfd044-12e3-471b-edb0-a6755f5a4d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view| avg_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.9819729328155518 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000? Order by descending view rating.\n",
        "# Although this is a small dataset, determine the run time for this query.\n",
        "import time\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the query\n",
        "result = spark.sql(\"\"\"SELECT view, ROUND(AVG(price), 2) as avg_price FROM home_sales GROUP BY view\n",
        "HAVING AVG(price) >= 350000 ORDER BY view DESC\"\"\")\n",
        "\n",
        "# Show the results\n",
        "result.show()\n",
        "\n",
        "# End timing and print the runtime\n",
        "end_time = time.time()\n",
        "print(\"--- %s seconds ---\" % (end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I cache the `home_sales` temporary table using `spark.catalog.cacheTable(\"home_sales\")`. This stores the table in memory, which can speed up future queries on this table.\n",
        "\n",
        "- After caching, I print \"Done\" to confirm that the operation was completed."
      ],
      "metadata": {
        "id": "bF2I0LuVyJKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KAhk3ZD2tFy8",
        "outputId": "d9f72be6-3da9-4b35-efec-e34d6ba6741a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.catalog.cacheTable(\"home_sales\")\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I check if the `home_sales` table is cached by using `spark.catalog.isCached('home_sales')` and store the result in the variable `cached_or_not`.\n",
        "\n",
        "- I use an `if` statement to see if `cached_or_not` is `True`.\n",
        "\n",
        "- If it is `True`, I print \"Yes! home_sales is cached\" to confirm that the table is indeed cached.\n",
        "\n",
        "- If it is `False`, I print \"Sorry! home_sales is not cached\" to indicate that the table is not cached."
      ],
      "metadata": {
        "id": "bS9ciSSdzGDg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4opVhbvxtL-i",
        "outputId": "e4167585-a6b0-4644-95cd-95f07a4fb48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Yes! home_sales is cached\n"
          ]
        }
      ],
      "source": [
        "# Check if the table 'home_sales' is cached\n",
        "cached_or_not = spark.catalog.isCached('home_sales')\n",
        "\n",
        "if cached_or_not:\n",
        "  print(\" Yes! home_sales is cached\")\n",
        "else:\n",
        "  print(\"Sorry! home_sales is not cached\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I start by importing the `time` module to measure the runtime of the cached query.\n",
        "\n",
        "- I use `time.time()` to get the current time before running the query, and store this in `start_time`.\n",
        "\n",
        "- I run the SQL query on the cached `home_sales` table using `spark.sql()`. This query calculates the average price of homes for each `view` rating, rounding the average price to two decimal places and including only those with an average price of $350,000 or more. The results are ordered by `view` in descending order.\n",
        "\n",
        "- I store the results of the cached query in `result_cached` and display them using `result_cached.show()`.\n",
        "\n",
        "- After running the query, I get the current time again using `time.time()` and store it in `end_time`.\n",
        "\n",
        "- I calculate the runtime by subtracting `start_time` from `end_time`, and print the runtime for the cached query.\n"
      ],
      "metadata": {
        "id": "vycnTIT40Vl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5GnL46lwTSEk",
        "outputId": "4c11de3d-303f-46c4-e852-c973c047913c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view| avg_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- Cached query runtime: 1.6546850204467773 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# Using the cached data, run the last query above, that calculates\n",
        "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000.\n",
        "# Determine the runtime and compare it to the uncached runtime.\n",
        "\n",
        "import time\n",
        "\n",
        "# Start timing for cached query\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the query on the cached data\n",
        "result_cached = spark.sql(\"\"\"SELECT view, ROUND(AVG(price), 2) as avg_price FROM home_sales\n",
        "GROUP BY view HAVING AVG(price) >= 350000 ORDER BY view DESC\"\"\")\n",
        "\n",
        "# Show the results\n",
        "result_cached.show()\n",
        "\n",
        "# End timing and print the runtime for cached query\n",
        "end_time = time.time()\n",
        "print(\"--- Cached query runtime: %s seconds ---\" % (end_time - start_time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I start by importing the `time` module to measure the runtime of both cached and uncached queries.\n",
        "\n",
        "- I use `time.time()` to capture the start time of the uncached query and store it in `start_time_uncached`.\n",
        "\n",
        "- I run the SQL query on the uncached `home_sales` table using `spark.sql()`. This query calculates the average price of homes for each `view` rating, rounding to two decimal places and filtering for average prices greater than or equal to $350,000. The results are ordered by `view` in descending order.\n",
        "\n",
        "- I store the results in `result_uncached` and display them with `result_uncached.show()`.\n",
        "\n",
        "- After running the uncached query, I capture the end time using `time.time()` and store it in `end_time_uncached`.\n",
        "\n",
        "- I calculate the runtime for the uncached query by subtracting `start_time_uncached` from `end_time_uncached`, and print the result.\n",
        "\n",
        "- Next, I cache the `home_sales` table using `spark.catalog.cacheTable(\"home_sales\")` to improve query performance for subsequent operations.\n",
        "\n",
        "- I then measure the runtime for the cached query. I capture the start time with `time.time()` and store it in `start_time_cached`.\n",
        "\n",
        "- I run the same SQL query on the cached `home_sales` table, store the results in `result_cached`, and display them using `result_cached.show()`.\n",
        "\n",
        "- After running the cached query, I capture the end time with `time.time()` and store it in `end_time_cached`.\n",
        "\n",
        "- I calculate the runtime for the cached query by subtracting `start_time_cached` from `end_time_cached`, and print the result.\n",
        "\n",
        "- Finally, I uncache the `home_sales` table using `spark.catalog.uncacheTable(\"home_sales\")` as a cleanup step to free up memory."
      ],
      "metadata": {
        "id": "YSIaIraT33vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Measure runtime for uncached query\n",
        "start_time_uncached = time.time()\n",
        "\n",
        "# Run the query on uncached data\n",
        "result_uncached = spark.sql(\"\"\"\n",
        "    SELECT view,\n",
        "           ROUND(AVG(price), 2) as avg_price\n",
        "    FROM home_sales\n",
        "    GROUP BY view\n",
        "    HAVING AVG(price) >= 350000\n",
        "    ORDER BY view DESC\n",
        "\"\"\")\n",
        "\n",
        "# Show the results\n",
        "result_uncached.show()\n",
        "\n",
        "# End timing and print the runtime for uncached query\n",
        "end_time_uncached = time.time()\n",
        "print(\"--- Uncached query runtime: %s seconds ---\" % (end_time_uncached - start_time_uncached))\n",
        "\n",
        "# Cache the table\n",
        "spark.catalog.cacheTable(\"home_sales\")\n",
        "\n",
        "# 2. Measure runtime for cached query\n",
        "start_time_cached = time.time()\n",
        "\n",
        "# Run the query on cached data\n",
        "result_cached = spark.sql(\"\"\"\n",
        "    SELECT view,\n",
        "           ROUND(AVG(price), 2) as avg_price\n",
        "    FROM home_sales\n",
        "    GROUP BY view\n",
        "    HAVING AVG(price) >= 350000\n",
        "    ORDER BY view DESC\n",
        "\"\"\")\n",
        "\n",
        "# Show the results\n",
        "result_cached.show()\n",
        "\n",
        "# End timing and print the runtime for cached query\n",
        "end_time_cached = time.time()\n",
        "print(\"--- Cached query runtime: %s seconds ---\" % (end_time_cached - start_time_cached))\n",
        "\n",
        "# Clean up by uncache the table (optional)\n",
        "spark.catalog.uncacheTable(\"home_sales\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9NRfe4qkH1tb",
        "outputId": "81cad03a-9fed-4f29-ae52-3fb7305e1abf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view| avg_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- Uncached query runtime: 0.4687016010284424 seconds ---\n",
            "+----+----------+\n",
            "|view| avg_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- Cached query runtime: 0.4913794994354248 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I write the DataFrame `df` to Parquet format, using partitioning based on the `date_built` field.\n",
        "\n",
        "- This is done with the `write.partitionBy(\"date_built\")` method, which organizes the data into separate folders for each unique `date_built` value.\n",
        "\n",
        "- The resulting Parquet files are saved to the specified path: `\"/dbfs/path/to/home_sales_partitioned.parquet\"`."
      ],
      "metadata": {
        "id": "wvQfQ37e5LAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "df.write.partitionBy(\"date_built\").parquet(\"/dbfs/path/to/home_sales_partitioned.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I read the Parquet data, which was partitioned by the `date_built` field, into a DataFrame named `df_parquet`.\n",
        "\n",
        "- I use `spark.read.parquet()` to load the data from the path `\"/dbfs/path/to/home_sales_partitioned.parquet\"`.\n",
        "\n",
        "- To verify that the data was loaded correctly, I display the schema of the DataFrame with `df_parquet.printSchema()`. This shows the structure of the DataFrame, including the column names and data types.\n",
        "\n",
        "- I also show the first few rows of the DataFrame using `df_parquet.show()`. This helps to quickly check the content and confirm that the data is as expected."
      ],
      "metadata": {
        "id": "Xv8c9PHc5QKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AZ7BgY61sRqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4c7d9915-a2a6-4beb-9f83-823bf2ff3996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- bedrooms: integer (nullable = true)\n",
            " |-- bathrooms: integer (nullable = true)\n",
            " |-- sqft_living: integer (nullable = true)\n",
            " |-- sqft_lot: integer (nullable = true)\n",
            " |-- floors: integer (nullable = true)\n",
            " |-- waterfront: integer (nullable = true)\n",
            " |-- view: integer (nullable = true)\n",
            " |-- date_built: integer (nullable = true)\n",
            "\n",
            "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
            "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
            "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
            "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
            "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
            "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
            "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
            "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
            "|a18515a2-86f3-46b...|2022-02-18|419543|       3|        2|       1642|   12826|     2|         0|  24|      2015|\n",
            "|98f6a9ad-8870-474...|2022-05-07|136752|       2|        3|       1701|   10771|     2|         0|   5|      2015|\n",
            "|7ac67498-b6f3-403...|2021-05-12|349318|       4|        3|       2417|   11304|     2|         0|  37|      2015|\n",
            "|c9bfdb1c-2499-4e3...|2021-12-07|268874|       2|        2|       1537|   12177|     1|         0|  10|      2015|\n",
            "|34c31a34-220d-469...|2019-02-06|409011|       3|        3|       2356|   10507|     1|         0|   1|      2015|\n",
            "|be0ccb95-415d-411...|2020-05-15|425154|       4|        3|       2120|   14229|     2|         0|   4|      2015|\n",
            "|e9031a86-1294-444...|2021-10-09|222322|       4|        3|       1928|   10510|     1|         0|  38|      2015|\n",
            "|e6d7c2a7-596e-4ec...|2019-03-15|131201|       4|        3|       1633|   14655|     1|         0|  22|      2015|\n",
            "|6683714b-3df7-454...|2022-02-01|333403|       4|        2|       2059|    9793|     2|         0|   4|      2015|\n",
            "|00fc996f-508c-430...|2021-07-15|373139|       3|        3|       1763|   11363|     1|         0|  39|      2015|\n",
            "|3d5545f8-bd3b-476...|2020-09-19|797862|       4|        6|       3494|   10385|     2|         0|  90|      2015|\n",
            "|ec6d357c-2435-43e...|2019-05-28|401792|       3|        2|       1627|   10765|     1|         0|  50|      2015|\n",
            "|c2be38fb-814a-403...|2020-03-20|352237|       3|        3|       2485|   10954|     2|         0|   6|      2015|\n",
            "|9570de1f-5a74-45b...|2021-11-29|298453|       3|        2|       2222|   10634|     1|         0|   6|      2015|\n",
            "|1baeff4f-fc00-489...|2020-12-17|152775|       3|        2|       1623|   13851|     1|         0|  41|      2015|\n",
            "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the Parquet data that is partitioned by 'date_built'\n",
        "df_parquet = spark.read.parquet(\"/dbfs/path/to/home_sales_partitioned.parquet\")\n",
        "\n",
        "# Show the schema and the first few rows to verify\n",
        "df_parquet.printSchema()\n",
        "df_parquet.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " I create a temporary view named `home_sales_parquet` from the DataFrame `df_parquet`.\n",
        "\n",
        "- This is done using `df_parquet.createOrReplaceTempView(\"home_sales_parquet\")`, which allows me to run SQL queries against this view."
      ],
      "metadata": {
        "id": "7LZKztcV5XMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# Create a temporary view from the DataFrame\n",
        "df_parquet.createOrReplaceTempView(\"home_sales_parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I start by measuring the runtime for a query executed on the Parquet data.\n",
        "\n",
        "- I use `time.time()` to record the start time.\n",
        "\n",
        "- I run the query on the temporary view `home_sales_parquet` using `spark.sql()`. The query calculates the average price of homes for each \"view\" rating, rounding the average price to two decimal places. It only includes homes with an average price greater than or equal to $350,000 and orders the results by view rating in descending order.\n",
        "\n",
        "- I show the results with `result_parquet.show()`.\n",
        "\n",
        "- I record the end time with `time.time()` and calculate the runtime by subtracting the start time from the end time.\n",
        "\n",
        "- Finally, I print the runtime for the query on the Parquet data."
      ],
      "metadata": {
        "id": "vw0uIsza5h3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "6afa30f8-0ab7-4e7b-fd8e-d5b4e905a365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view| avg_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- Parquet query runtime: 1.1789438724517822 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 13. Using the parquet DataFrame, run the last query above, that calculates\n",
        "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000.\n",
        "# Determine the runtime and compare it to the cached runtime.\n",
        "start_time = time.time()\n",
        "\n",
        "result_parquet = spark.sql(\"\"\"SELECT view,ROUND(AVG(price), 2) as avg_price FROM home_sales_parquet\n",
        "GROUP BY view HAVING AVG(price) >= 350000 ORDER BY view DESC\"\"\")\n",
        "\n",
        "# Show the results\n",
        "result_parquet.show()\n",
        "\n",
        "# End timing and print the runtime for the query on the Parquet data\n",
        "end_time = time.time()\n",
        "print(\"--- Parquet query runtime: %s seconds ---\" % (end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I uncache the `home_sales` temporary table using `spark.catalog.uncacheTable(\"home_sales\")`.\n",
        "\n",
        "- This command removes the cached data from memory, freeing up resources and ensuring that future queries will not use the cached version of the table."
      ],
      "metadata": {
        "id": "crvAG7OY5p4Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hjjYzQGjtbq8"
      },
      "outputs": [],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.catalog.uncacheTable(\"home_sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I check if the `home_sales` table is still cached by using `spark.catalog.isCached(\"home_sales\")`.\n",
        "\n",
        "- I store the result in the variable `is_cached`.\n",
        "\n",
        "- I then print whether the `home_sales` table is cached or not."
      ],
      "metadata": {
        "id": "Ng0UB6mX5tRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Sy9NBvO7tlmm",
        "outputId": "0b89d6cc-b55e-4bb0-ef05-6cb8a51c82f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is 'home_sales' cached?: False\n"
          ]
        }
      ],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "is_cached = spark.catalog.isCached(\"home_sales\")\n",
        "\n",
        "# Print the result\n",
        "print(\"Is 'home_sales' cached?:\", is_cached)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What I learned from this module?\n",
        "In this exercise, I learned how to manage and optimize data processing in Spark using Parquet files. The process began with reading CSV data into a DataFrame and then partitioning it by the `date_built` field before saving it in Parquet format. I created temporary tables from both CSV and Parquet data to run SQL queries. It included calculating average home prices based on various criteria. I evaluated the performance benefits of caching by comparing runtimes for queries on cached and uncached data. Finally, I learned how to uncache tables and verify their cache status."
      ],
      "metadata": {
        "id": "a5XCgp0l51q6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}